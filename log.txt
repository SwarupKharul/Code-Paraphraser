

Calling tokenize_lines
Tokens: # This is a comment
def example_function(param1 = 60, param2 = 50):
    
    string = " this is a string "
    print(param1)
    print(param2)

Calling filter_comments
Tokens: ['# This is a comment', 'def example_function(param1 = 60, param2 = 50):', 'string = " this is a string "', 'print(param1)', 'print(param2)']

Calling filter_strings
Tokens: ['def example_function(param1 = 60, param2 = 50):', 'string = " this is a string "', 'print(param1)', 'print(param2)']
string = " this is a string " replaced with string =  

Tokens: ['def example_function(param1 = 60, param2 = 50):', 'string =  ', 'print(param1)', 'print(param2)']

Calling tokenize_words
Tokens: ['def example_function(param1 = 60, param2 = 50):', 'string =  ', 'print(param1)', 'print(param2)']

Calling filter_empty_tokens
Tokens: ['def', 'example_function', 'param1', '60', 'param2', '50', 'string', 'print', 'param1', 'print', 'param2']

Calling filter_numbers
Tokens: ['def', 'example_function', 'param1', '60', 'param2', '50', 'string', 'print', 'param1', 'print', 'param2']

Calling filter_keywords
Tokens: ['def', 'example_function', 'param1', 'param2', 'string', 'print', 'param1', 'print', 'param2']